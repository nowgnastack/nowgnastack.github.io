<!DOCTYPE html>
<html lang=en>
<head>
  <!-- so meta -->
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
  <meta name="google-site-verification" content="sdTAxKN3zLoy8rmPlkLzOu030Yo_stjU8dw_ufxbSoA" />
  <meta name="description" content="Derivative 미분은 한 순간의 변화량을 표현한 것이다.  \begin{align}\Delta y &#x3D; \alpha \Delta x\end{align}  $\alpha$(기울기)와 x변화량의 곱은 y의 변화량과 같다. 일반적인 최적화 문제는 f(x)의 값이 가장 작아지거나 가장 커지는 x를 찾는 것이다.    Gradient Descent 기울">
<meta property="og:type" content="article">
<meta property="og:title" content="Neural Network II">
<meta property="og:url" content="https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/index.html">
<meta property="og:site_name" content="nowgnastack">
<meta property="og:description" content="Derivative 미분은 한 순간의 변화량을 표현한 것이다.  \begin{align}\Delta y &#x3D; \alpha \Delta x\end{align}  $\alpha$(기울기)와 x변화량의 곱은 y의 변화량과 같다. 일반적인 최적화 문제는 f(x)의 값이 가장 작아지거나 가장 커지는 x를 찾는 것이다.    Gradient Descent 기울">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://nowgnastack.github.io/images/img/posts/TaveResearch/neuralN2/derivative.png">
<meta property="og:image" content="https://nowgnastack.github.io/images/img/posts/TaveResearch/neuralN2/gradDescent.png">
<meta property="og:image" content="https://nowgnastack.github.io/images/img/posts/TaveResearch/neuralN2/gradientDescent.png">
<meta property="og:image" content="https://nowgnastack.github.io/images/img/posts/TaveResearch/neuralN2/softmax.png">
<meta property="article:published_time" content="2021-01-01T15:00:00.000Z">
<meta property="article:modified_time" content="2023-07-23T15:06:36.672Z">
<meta property="article:author" content="nowgnas">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="tave">
<meta property="article:tag" content="tave research">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://nowgnastack.github.io/images/img/posts/TaveResearch/neuralN2/derivative.png">    
  <link rel="shortcut icon" href="/images/favicon.ico" />
     
  <link
    rel="icon"
    type="image/png"
    href="/images/favicon-192x192.png"
    sizes="192x192"
  />
     
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png" />
    
  <!-- title -->
  <title>Neural Network II</title>
  <!-- async scripts -->
  <!-- Google Analytics -->

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-HS37DQYSPL"></script>
  <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-HS37DQYSPL');
  </script>

 <!-- Umami Analytics -->


  <!-- styles -->
  
<link rel="stylesheet" href="/css/style.css">

  <!-- persian styles -->
  
  <!-- rss -->
   
  <!-- mathjax -->
  
  <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
     });
  </script>
  <script
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"
    async
  ></script>
  
  <!--Canonical : 유사하거나 중복된 페이지의 표준 페이지 정의-->
  <link rel="canonical" href="https://nowgnastack.github.io/2021/01/02/2021-01-02-tave_research2/"/>
<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="nowgnastack" type="application/atom+xml">
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="https://github.com/nowgnas">Projects</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2021/01/03/2021-01-03-TAVE_Research3/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2020/12/30/2020-12-30-TAVE_Research1/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/&text=Neural Network II"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/&title=Neural Network II"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/&is_video=false&description=Neural Network II"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Neural Network II&body=Check out this article: https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/&title=Neural Network II"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/&title=Neural Network II"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/&title=Neural Network II"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/&title=Neural Network II"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/&name=Neural Network II&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/&t=Neural Network II"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Derivative"><span class="toc-number">1.</span> <span class="toc-text">Derivative</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Descent"><span class="toc-number">2.</span> <span class="toc-text">Gradient Descent</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EA%B2%BD%EC%82%AC%EB%B2%95%EC%97%90-%EC%9D%98%ED%95%9C-f-x-0-x-1-x-0-2-x-1-2-%ED%95%A8%EC%88%98%EC%9D%98-%EA%B0%B1%EC%8B%A0-%EA%B3%BC%EC%A0%95"><span class="toc-number">2.1.</span> <span class="toc-text">경사법에 의한 $f(x_0, x_1) &#x3D; (x_0)^2 + (x_1)^2$ 함수의 갱신 과정</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Softmax-Function"><span class="toc-number">3.</span> <span class="toc-text">Softmax Function</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#References"><span class="toc-number">3.0.0.0.1.</span> <span class="toc-text">References</span></a></li></ol></li></ol></li></ol></li></ol></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        Neural Network II
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">nowgnas</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2021-01-01T15:00:00.000Z" class="dt-published" itemprop="datePublished">2021-01-02</time>
        
      
    </div>


      
<div class="article-category">
  <i class="fa-solid fa-archive"></i>
  <a class="category-link" href="/categories/deep-learning/">deep learning</a>
</div>


      
    <div class="article-tag">
        <i class="fa-solid fa-tag"></i>
        <a class="p-category" href="/tags/deep-learning/" rel="tag">deep learning</a>, <a class="p-category" href="/tags/tave/" rel="tag">tave</a>, <a class="p-category" href="/tags/tave-research/" rel="tag">tave research</a>
    </div>


    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <h2 id="Derivative"><a href="#Derivative" class="headerlink" title="Derivative"></a>Derivative</h2><ul>
<li><p>미분은 한 순간의 변화량을 표현한 것이다.</p>
<p><img src="/images/img/posts/TaveResearch/neuralN2/derivative.png" alt="derivative.png"></p>
<p>\begin{align}<br>\Delta y &#x3D; \alpha \Delta x<br>\end{align}</p>
<ul>
<li>$\alpha$(기울기)와 x변화량의 곱은 y의 변화량과 같다.</li>
<li>일반적인 최적화 문제는 f(x)의 값이 가장 작아지거나 가장 커지는 x를 찾는 것이다.</li>
</ul>
</li>
</ul>
<h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><ul>
<li><p>기울기를 이용하여 함수의 최솟값이나 최댓값을 찾는다.</p>
</li>
<li><p>함수가 볼록한 형태에서는 항상 경사 하강법으로 최솟값을 찾을 수 있다. (a)</p>
</li>
<li><p>볼록하지 않은 함수에서는 해당 함수의 극솟값이나 안정점에 도달 할 수 있다. (b)</p>
<p><img src="/images/img/posts/TaveResearch/neuralN2/gradDescent.png" alt="gradDescent.png"></p>
<h3 id="경사법에-의한-f-x-0-x-1-x-0-2-x-1-2-함수의-갱신-과정"><a href="#경사법에-의한-f-x-0-x-1-x-0-2-x-1-2-함수의-갱신-과정" class="headerlink" title="경사법에 의한 $f(x_0, x_1) &#x3D; (x_0)^2 + (x_1)^2$ 함수의 갱신 과정"></a>경사법에 의한 $f(x_0, x_1) &#x3D; (x_0)^2 + (x_1)^2$ 함수의 갱신 과정</h3><p>\begin{align}<br>x_0 &#x3D; \eta ({\partial f &#x2F; \partial x_0})<br>\end{align}</p>
<ul>
<li>$\eta$는 갱신하는 양을 나타낸다. 학습률(learning rate)이다.</li>
<li>numerical_gradient 함수는 함수의 기울기를 반환해 준다.</li>
<li>gradient_descent 함수에서 init_x는 초기값, lr은 learning rate, step_num은 경사법에 따른 반복 횟수다.</li>
<li>function_2는 함수 $f(x_0,x_1)$이다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">numerical_gradient</span>(<span class="params">f, x</span>):</span><br><span class="line">    h = <span class="number">1e-4</span></span><br><span class="line">    grad = np.zeros_like(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(x.size):</span><br><span class="line">        tmp_val = x[idx]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># f(x+h)</span></span><br><span class="line">        x[idx] = tmp_val + h</span><br><span class="line">        fxh1 = f(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># f(x-h)</span></span><br><span class="line">        x[idx] = tmp_val - h</span><br><span class="line">        fxh2 = f(x)</span><br><span class="line"></span><br><span class="line">        grad[idx] = (fxh1 - fxh2) / (<span class="number">2</span> * h)</span><br><span class="line">        x[idx] = tmp_val</span><br><span class="line">    <span class="keyword">return</span> grad</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">f, init_x, lr=<span class="number">0.01</span>, step_num=<span class="number">100</span></span>):</span><br><span class="line">    x = init_x</span><br><span class="line">    x_history = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(step_num):</span><br><span class="line">        x_history.append(x.copy())</span><br><span class="line"></span><br><span class="line">        grad = numerical_gradient(f, x)</span><br><span class="line">        x -= lr * grad</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, np.array(x_history)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">function_2</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x[<span class="number">0</span>] ** <span class="number">2</span> + x[<span class="number">1</span>] ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">init_x = np.array([-<span class="number">3.0</span>, <span class="number">4.0</span>])</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">step_num = <span class="number">20</span></span><br><span class="line">x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)</span><br><span class="line"></span><br><span class="line">plt.plot([-<span class="number">5</span>, <span class="number">5</span>], [<span class="number">0</span>, <span class="number">0</span>], <span class="string">&#x27;--b&#x27;</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">0</span>], [-<span class="number">5</span>, <span class="number">5</span>], <span class="string">&#x27;--b&#x27;</span>)</span><br><span class="line">plt.plot(x_history[:, <span class="number">0</span>], x_history[:, <span class="number">1</span>], <span class="string">&#x27;o&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(-<span class="number">3.5</span>, <span class="number">3.5</span>)</span><br><span class="line">plt.ylim(-<span class="number">4.5</span>, <span class="number">4.5</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;X0&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;X1&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/images/img/posts/TaveResearch/neuralN2/gradientDescent.png" alt="gradientDescent.png"></p>
<ul>
<li>경사법을 사용한 갱신 과정을 나타낸 그림으로 최솟값인 원점에 점점 가까워지는 것을 볼 수 있다.</li>
</ul>
</li>
</ul>
<h2 id="Softmax-Function"><a href="#Softmax-Function" class="headerlink" title="Softmax Function"></a>Softmax Function</h2><p><img src="/images/img/posts/TaveResearch/neuralN2/softmax.png" alt="softmax.png"></p>
<p>\begin{align}<br>{y_k} &#x3D; \exp(a_k) &#x2F; {\sum{^n}}_{i&#x3D;1} {\exp(a_i)}<br>\end{align}</p>
<ul>
<li>n은 출력층의 뉴런 수, $y_k$는 그중 k번째 출력을 뜻한다.</li>
<li>softmax 함수는 분류해야 하는 클래스의 총 개수를 a라고 할 때 b차원의 벡터를 입력 받아 각 클래스에 대한 확률을 추정한다</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># softmax function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">a</span>):</span><br><span class="line">    c = np.<span class="built_in">max</span>(a)</span><br><span class="line">    exp_a = np.exp(a - c)</span><br><span class="line">    sum_exp_a = np.<span class="built_in">sum</span>(exp_a)</span><br><span class="line">    y = exp_a / sum_exp_a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">np.array([<span class="number">0.3</span>, <span class="number">2.9</span>, <span class="number">4.0</span>])</span><br><span class="line"><span class="comment"># array([0.3, 2.9, 4. ])</span></span><br><span class="line"></span><br><span class="line">softmax(a)</span><br><span class="line"><span class="comment"># array([0.01821127, 0.24519181, 0.73659691])</span></span><br><span class="line"></span><br><span class="line">np.<span class="built_in">sum</span>(res)</span><br><span class="line"><span class="comment"># 1.0</span></span><br></pre></td></tr></table></figure>

<ul>
<li>softmax 함수의 출력은 0과 1사이의 실수다.</li>
<li>softmax 함수의 출력의 총합은 1이다.</li>
</ul>
<h6 id="References"><a href="#References" class="headerlink" title="References"></a>References</h6><blockquote>
<p><sub>강의: <a target="_blank" rel="noopener" href="https://deeplearning.cs.cmu.edu/F20/index.html">CMU Introduction to Deep Learning</a></sub><br><sub>코드: <a target="_blank" rel="noopener" href="https://www.hanbit.co.kr/store/books/look.php?p_code=B8475831198">밑바닥부터 시작하는 딥러닝</a></sub></p>
</blockquote>

  </div>
</article>


    <div class="blog-post-comments">
        <div id="utterances_thread">
            <noscript>Please enable JavaScript to view the comments.</noscript>
        </div>
    </div>


        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/about/">About</a></li>
        
          <li><a href="/archives/">Writing</a></li>
        
          <li><a target="_blank" rel="noopener" href="https://github.com/nowgnas">Projects</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Derivative"><span class="toc-number">1.</span> <span class="toc-text">Derivative</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Descent"><span class="toc-number">2.</span> <span class="toc-text">Gradient Descent</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EA%B2%BD%EC%82%AC%EB%B2%95%EC%97%90-%EC%9D%98%ED%95%9C-f-x-0-x-1-x-0-2-x-1-2-%ED%95%A8%EC%88%98%EC%9D%98-%EA%B0%B1%EC%8B%A0-%EA%B3%BC%EC%A0%95"><span class="toc-number">2.1.</span> <span class="toc-text">경사법에 의한 $f(x_0, x_1) &#x3D; (x_0)^2 + (x_1)^2$ 함수의 갱신 과정</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Softmax-Function"><span class="toc-number">3.</span> <span class="toc-text">Softmax Function</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#References"><span class="toc-number">3.0.0.0.1.</span> <span class="toc-text">References</span></a></li></ol></li></ol></li></ol></li></ol></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/&text=Neural Network II"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/&title=Neural Network II"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/&is_video=false&description=Neural Network II"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Neural Network II&body=Check out this article: https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/&title=Neural Network II"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/&title=Neural Network II"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/&title=Neural Network II"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/&title=Neural Network II"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/&name=Neural Network II&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://nowgnastack.github.io/2021/01/02/2021-01-02-TAVE_Research2/&t=Neural Network II"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2020-2023
    nowgnas
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="https://github.com/nowgnas">Projects</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

    <script type="text/javascript">
      var utterances_repo = 'nowgnastack/nowgnastack.github.io';
      var utterances_issue_term = 'pathname';
      var utterances_label = 'Comment';
      var utterances_theme = 'github-light';

      (function(){
          var script = document.createElement('script');

          script.src = 'https://utteranc.es/client.js';
          script.setAttribute('repo', utterances_repo);
          script.setAttribute('issue-term', 'pathname');
          script.setAttribute('label', utterances_label);
          script.setAttribute('theme', utterances_theme);
          script.setAttribute('crossorigin', 'anonymous');
          script.async = true;
          (document.getElementById('utterances_thread')).appendChild(script);
      }());
  </script>

</body>
</html>
